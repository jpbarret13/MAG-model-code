{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZJD011yhyWjD",
    "outputId": "35199005-d2bf-4120-bc5e-1e6fb8797d82"
   },
   "outputs": [],
   "source": [
    "# %pip install tensorflow==2.4.1\n",
    "# %pip install transformers\n",
    "# %pip install pyarrow\n",
    "# %pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oV5qIlEokph9"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import tensorflow_addons as tfa\n",
    "from math import ceil\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from transformers import RobertaTokenizer, RobertaTokenizerFast, TFRobertaModel, TFAlbertModel\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_iteration = 'iteration_final/basic_word_tokenized_500_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9I2c4qh5FZa-"
   },
   "outputs": [],
   "source": [
    "with open(f\"./{model_iteration}/vocab/topics_vocab.pkl\", \"rb\") as f:\n",
    "    target_vocab = pickle.load(f)\n",
    "    \n",
    "target_vocab_inv = {j:i for i,j in target_vocab.items()}\n",
    "\n",
    "with open(f\"./{model_iteration}/vocab/doc_type_vocab.pkl\", \"rb\") as f:\n",
    "    doc_vocab = pickle.load(f)\n",
    "    \n",
    "doc_vocab_inv = {j:i for i,j in doc_vocab.items()}\n",
    "\n",
    "with open(f\"./{model_iteration}/vocab/journal_name_vocab.pkl\", \"rb\") as f:\n",
    "    journal_vocab = pickle.load(f)\n",
    "    \n",
    "journal_vocab_inv = {j:i for i,j in journal_vocab.items()}\n",
    "\n",
    "with open(f\"./{model_iteration}/vocab/paper_title_vocab.pkl\", \"rb\") as f:\n",
    "    title_vocab = pickle.load(f)\n",
    "    \n",
    "title_vocab_inv = {j:i for i,j in title_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8dvSQiNsFHr4"
   },
   "outputs": [],
   "source": [
    "encoding_layer = tf.keras.layers.experimental.preprocessing.CategoryEncoding(\n",
    "    max_tokens=len(target_vocab)+1, output_mode=\"binary\", sparse=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177567"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(title_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82178"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-25 23:06:01.068679: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-11-25 23:06:01.077543: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "mag_model = tf.keras.models.load_model(f'./{model_iteration}/models/gamma_28_nH8_nL6_epoch25/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 64) dtype=int64 (created by layer 'paper_title_ids')>,\n",
       " <KerasTensor: shape=(None, 1) dtype=int64 (created by layer 'doc_type_id')>,\n",
       " <KerasTensor: shape=(None, 1) dtype=int64 (created by layer 'journal_id')>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mag_model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = tf.keras.Model(inputs=mag_model.inputs, \n",
    "                             outputs=tf.math.top_k(mag_model.outputs, k=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "paper_title_ids (InputLayer)    [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_embedding (Embedding)     (None, 64, 512)      90914816    paper_title_ids[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 64, 512)      0           title_embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/multiheadattention (M (None, 64, 512)      1050624     tf.__operators__.add[0][0]       \n",
      "                                                                 tf.__operators__.add[0][0]       \n",
      "                                                                 tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/att_dropout (Dropout) (None, 64, 512)      0           encoder_0/multiheadattention[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_1 (TFOpLam (None, 64, 512)      0           tf.__operators__.add[0][0]       \n",
      "                                                                 encoder_0/att_dropout[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/att_layernormalizatio (None, 64, 512)      1024        tf.__operators__.add_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/ffn (Sequential)      (None, 64, 512)      1050112     encoder_0/att_layernormalization[\n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/ffn_dropout (Dropout) (None, 64, 512)      0           encoder_0/ffn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_2 (TFOpLam (None, 64, 512)      0           encoder_0/att_layernormalization[\n",
      "                                                                 encoder_0/ffn_dropout[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/ffn_layernormalizatio (None, 64, 512)      1024        tf.__operators__.add_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "encoder_1/multiheadattention (M (None, 64, 512)      1050624     encoder_0/ffn_layernormalization[\n",
      "                                                                 encoder_0/ffn_layernormalization[\n",
      "                                                                 encoder_0/ffn_layernormalization[\n",
      "__________________________________________________________________________________________________\n",
      "encoder_1/att_dropout (Dropout) (None, 64, 512)      0           encoder_1/multiheadattention[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_3 (TFOpLam (None, 64, 512)      0           encoder_0/ffn_layernormalization[\n",
      "                                                                 encoder_1/att_dropout[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "encoder_1/att_layernormalizatio (None, 64, 512)      1024        tf.__operators__.add_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "encoder_1/ffn (Sequential)      (None, 64, 512)      1050112     encoder_1/att_layernormalization[\n",
      "__________________________________________________________________________________________________\n",
      "encoder_1/ffn_dropout (Dropout) (None, 64, 512)      0           encoder_1/ffn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_4 (TFOpLam (None, 64, 512)      0           encoder_1/att_layernormalization[\n",
      "                                                                 encoder_1/ffn_dropout[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "encoder_1/ffn_layernormalizatio (None, 64, 512)      1024        tf.__operators__.add_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "encoder_2/multiheadattention (M (None, 64, 512)      1050624     encoder_1/ffn_layernormalization[\n",
      "                                                                 encoder_1/ffn_layernormalization[\n",
      "                                                                 encoder_1/ffn_layernormalization[\n",
      "__________________________________________________________________________________________________\n",
      "encoder_2/att_dropout (Dropout) (None, 64, 512)      0           encoder_2/multiheadattention[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_5 (TFOpLam (None, 64, 512)      0           encoder_1/ffn_layernormalization[\n",
      "                                                                 encoder_2/att_dropout[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "encoder_2/att_layernormalizatio (None, 64, 512)      1024        tf.__operators__.add_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "encoder_2/ffn (Sequential)      (None, 64, 512)      1050112     encoder_2/att_layernormalization[\n",
      "__________________________________________________________________________________________________\n",
      "encoder_2/ffn_dropout (Dropout) (None, 64, 512)      0           encoder_2/ffn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_6 (TFOpLam (None, 64, 512)      0           encoder_2/att_layernormalization[\n",
      "                                                                 encoder_2/ffn_dropout[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "encoder_2/ffn_layernormalizatio (None, 64, 512)      1024        tf.__operators__.add_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "encoder_3/multiheadattention (M (None, 64, 512)      1050624     encoder_2/ffn_layernormalization[\n",
      "                                                                 encoder_2/ffn_layernormalization[\n",
      "                                                                 encoder_2/ffn_layernormalization[\n",
      "__________________________________________________________________________________________________\n",
      "encoder_3/att_dropout (Dropout) (None, 64, 512)      0           encoder_3/multiheadattention[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_7 (TFOpLam (None, 64, 512)      0           encoder_2/ffn_layernormalization[\n",
      "                                                                 encoder_3/att_dropout[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "encoder_3/att_layernormalizatio (None, 64, 512)      1024        tf.__operators__.add_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "encoder_3/ffn (Sequential)      (None, 64, 512)      1050112     encoder_3/att_layernormalization[\n",
      "__________________________________________________________________________________________________\n",
      "encoder_3/ffn_dropout (Dropout) (None, 64, 512)      0           encoder_3/ffn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_8 (TFOpLam (None, 64, 512)      0           encoder_3/att_layernormalization[\n",
      "                                                                 encoder_3/ffn_dropout[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "encoder_3/ffn_layernormalizatio (None, 64, 512)      1024        tf.__operators__.add_8[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "encoder_4/multiheadattention (M (None, 64, 512)      1050624     encoder_3/ffn_layernormalization[\n",
      "                                                                 encoder_3/ffn_layernormalization[\n",
      "                                                                 encoder_3/ffn_layernormalization[\n",
      "__________________________________________________________________________________________________\n",
      "encoder_4/att_dropout (Dropout) (None, 64, 512)      0           encoder_4/multiheadattention[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_9 (TFOpLam (None, 64, 512)      0           encoder_3/ffn_layernormalization[\n",
      "                                                                 encoder_4/att_dropout[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "encoder_4/att_layernormalizatio (None, 64, 512)      1024        tf.__operators__.add_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "encoder_4/ffn (Sequential)      (None, 64, 512)      1050112     encoder_4/att_layernormalization[\n",
      "__________________________________________________________________________________________________\n",
      "encoder_4/ffn_dropout (Dropout) (None, 64, 512)      0           encoder_4/ffn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_10 (TFOpLa (None, 64, 512)      0           encoder_4/att_layernormalization[\n",
      "                                                                 encoder_4/ffn_dropout[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "encoder_4/ffn_layernormalizatio (None, 64, 512)      1024        tf.__operators__.add_10[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "encoder_5/multiheadattention (M (None, 64, 512)      1050624     encoder_4/ffn_layernormalization[\n",
      "                                                                 encoder_4/ffn_layernormalization[\n",
      "                                                                 encoder_4/ffn_layernormalization[\n",
      "__________________________________________________________________________________________________\n",
      "encoder_5/att_dropout (Dropout) (None, 64, 512)      0           encoder_5/multiheadattention[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_11 (TFOpLa (None, 64, 512)      0           encoder_4/ffn_layernormalization[\n",
      "                                                                 encoder_5/att_dropout[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "encoder_5/att_layernormalizatio (None, 64, 512)      1024        tf.__operators__.add_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "encoder_5/ffn (Sequential)      (None, 64, 512)      1050112     encoder_5/att_layernormalization[\n",
      "__________________________________________________________________________________________________\n",
      "encoder_5/ffn_dropout (Dropout) (None, 64, 512)      0           encoder_5/ffn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_12 (TFOpLa (None, 64, 512)      0           encoder_5/att_layernormalization[\n",
      "                                                                 encoder_5/ffn_dropout[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "encoder_5/ffn_layernormalizatio (None, 64, 512)      1024        tf.__operators__.add_12[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64, 2048)     1050624     encoder_5/ffn_layernormalization[\n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64, 1024)     525312      title_embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64, 2048)     0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64, 1024)     0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "journal_id (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "doc_type_id (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "layer_norm_1 (LayerNormalizatio (None, 64, 2048)     4096        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_norm_2 (LayerNormalizatio (None, 64, 1024)     2048        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "journal_embedding (Embedding)   (None, 1, 64)        2739904     journal_id[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "doc_type_embedding (Embedding)  (None, 1, 32)        288         doc_type_id[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "title_pooling_layer (GlobalAver (None, 2048)         0           layer_norm_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "title_pooling_layer2 (GlobalAve (None, 1024)         0           layer_norm_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "journal_pooling_layer (GlobalAv (None, 64)           0           journal_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "doc_pooling_layer (GlobalAverag (None, 32)           0           doc_type_embedding[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (None, 3168)         0           title_pooling_layer[0][0]        \n",
      "                                                                 title_pooling_layer2[0][0]       \n",
      "                                                                 journal_pooling_layer[0][0]      \n",
      "                                                                 doc_pooling_layer[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1024)         3245056     tf.concat[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1024)         0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_norm_3 (LayerNormalizatio (None, 1024)         2048        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "cls (Dense)                     (None, 82179)        84233475    layer_norm_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.top_k (TFOpLambda)      TopKV2(values=(1, No 0           cls[0][0]                        \n",
      "==================================================================================================\n",
      "Total params: 195,334,371\n",
      "Trainable params: 195,334,371\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_model_predictions(data_path):\n",
    "    # Get all of the files, load into single pandas dataframe\n",
    "    # split up into blocks of 3000 and get model output\n",
    "    file_names = [x for x in os.listdir(f\"./{model_iteration}/tokenized_data/test/\") if x.startswith('part')]\n",
    "    file_names.sort()\n",
    "    \n",
    "    full_df = pd.DataFrame()\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        temp_df = pd.read_parquet(f\"./{model_iteration}/tokenized_data/test/{file_name}\")\n",
    "        full_df = pd.concat([full_df, temp_df], axis=0)\n",
    "    \n",
    "    num_samples = 1000\n",
    "    preds_final = []\n",
    "    scores_final = []\n",
    "    for i in range(ceil(full_df.shape[0]/num_samples)):\n",
    "        print(i)\n",
    "        small_df = full_df.iloc[i*num_samples:(i+1)*num_samples, :].copy()\n",
    "        preds, scores = get_model_predictions(small_df)\n",
    "        preds_final += preds\n",
    "        scores_final += scores\n",
    "    \n",
    "    full_df['predictions'] = preds_final\n",
    "    full_df['scores'] = scores_final\n",
    "    \n",
    "    return full_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_predictions(input_data):\n",
    "    \n",
    "    paper_titles = tf.keras.preprocessing.sequence.pad_sequences(input_data['paper_title_tok'].to_list(), maxlen=64, \n",
    "                                                             dtype='int64', padding='post', \n",
    "                                                             truncating='post', value=0)\n",
    "    \n",
    "    doc_types = tf.convert_to_tensor(input_data['doc_type_tok'].to_list())\n",
    "    journal = tf.convert_to_tensor(input_data['journal_tok'].to_list())\n",
    "    \n",
    "    model_output = final_model([paper_titles, doc_types, journal])\n",
    "    \n",
    "    scores = model_output.values.numpy()[0][:,:20].tolist()\n",
    "    preds = model_output.indices.numpy()[0][:,:20].tolist()1`\n",
    "    \n",
    "    return preds, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n"
     ]
    }
   ],
   "source": [
    "test_data = get_all_model_predictions(f\"./{model_iteration}/tokenized_data/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_parquet(f\"./{model_iteration}/test_data/data_with_predictions_500_gamma28_nH8_nL6_epoch25.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = pd.read_parquet(f\"./{model_iteration}/test_data/data_with_predictions_500.parquet\")\n",
    "test_data['target_test'] = test_data['target_tok'].apply(lambda x: [i for i in x if i!=-1])\n",
    "test_data['target_test'] = test_data['target_test'].apply(len)\n",
    "test_data = test_data[test_data['target_test'] > 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97233, 9)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>doc_type_tok</th>\n",
       "      <th>journal_tok</th>\n",
       "      <th>target_tok</th>\n",
       "      <th>paper_title_tok</th>\n",
       "      <th>predictions</th>\n",
       "      <th>scores</th>\n",
       "      <th>target_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23907</th>\n",
       "      <td>3131361924</td>\n",
       "      <td>2021-07-01</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[294]</td>\n",
       "      <td>[1715, 101, 2, 1381]</td>\n",
       "      <td>[168800, 166817, 107386, 126558, 151334, 17634...</td>\n",
       "      <td>[2, 1381, 42, 171, 37, 306, 156, 11791, 543, 1...</td>\n",
       "      <td>[0.641745388507843, 0.4675076901912689, 0.4003...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1629</th>\n",
       "      <td>2173313759</td>\n",
       "      <td>1971-12-01</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[2775]</td>\n",
       "      <td>[2076, 8, 2685, 49, 3770, 388, 239, 2736, 1524...</td>\n",
       "      <td>[161506, 64102, 37508, 126553, 97706, 151326, ...</td>\n",
       "      <td>[8, 49, 239, 95, 696, 2736, 2685, 685, 5727, 3...</td>\n",
       "      <td>[0.7249653339385986, 0.4582909345626831, 0.438...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4440</th>\n",
       "      <td>2357299977</td>\n",
       "      <td>2013-03-01</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[1039]</td>\n",
       "      <td>[1425, 5, 328, 81, 691, 1835, 478, 586, 251, 8...</td>\n",
       "      <td>[35688, 633, 117665, 162290, 14251, 126553, 13...</td>\n",
       "      <td>[5, 8493, 586, 691, 328, 251, 623, 42, 397, 47...</td>\n",
       "      <td>[0.6601946353912354, 0.6510505080223083, 0.572...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8505</th>\n",
       "      <td>2747201174</td>\n",
       "      <td>2017-09-29</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[5623]</td>\n",
       "      <td>[61, 1, 38146, 443]</td>\n",
       "      <td>[103617, 14805, 112377, 131258, 152551, 150768...</td>\n",
       "      <td>[1, 6, 31, 2024, 38146, 2181, 61, 76, 85, 1967...</td>\n",
       "      <td>[0.9602290391921997, 0.5080435276031494, 0.466...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18728</th>\n",
       "      <td>3134951874</td>\n",
       "      <td>2021-06-01</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[2384]</td>\n",
       "      <td>[774, 2, 37, 16104, 14212, 3972, 215, 4050, 79...</td>\n",
       "      <td>[100107, 17828, 126553, 53496, 133601, 166817,...</td>\n",
       "      <td>[79, 1708, 2, 774, 1139, 215, 14212, 44, 29434...</td>\n",
       "      <td>[0.8013430237770081, 0.688534677028656, 0.6055...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         paper_id publication_date doc_type_tok journal_tok  \\\n",
       "23907  3131361924       2021-07-01          [3]       [294]   \n",
       "1629   2173313759       1971-12-01          [3]      [2775]   \n",
       "4440   2357299977       2013-03-01          [3]      [1039]   \n",
       "8505   2747201174       2017-09-29          [3]      [5623]   \n",
       "18728  3134951874       2021-06-01          [3]      [2384]   \n",
       "\n",
       "                                              target_tok  \\\n",
       "23907                               [1715, 101, 2, 1381]   \n",
       "1629   [2076, 8, 2685, 49, 3770, 388, 239, 2736, 1524...   \n",
       "4440   [1425, 5, 328, 81, 691, 1835, 478, 586, 251, 8...   \n",
       "8505                                 [61, 1, 38146, 443]   \n",
       "18728  [774, 2, 37, 16104, 14212, 3972, 215, 4050, 79...   \n",
       "\n",
       "                                         paper_title_tok  \\\n",
       "23907  [168800, 166817, 107386, 126558, 151334, 17634...   \n",
       "1629   [161506, 64102, 37508, 126553, 97706, 151326, ...   \n",
       "4440   [35688, 633, 117665, 162290, 14251, 126553, 13...   \n",
       "8505   [103617, 14805, 112377, 131258, 152551, 150768...   \n",
       "18728  [100107, 17828, 126553, 53496, 133601, 166817,...   \n",
       "\n",
       "                                             predictions  \\\n",
       "23907  [2, 1381, 42, 171, 37, 306, 156, 11791, 543, 1...   \n",
       "1629   [8, 49, 239, 95, 696, 2736, 2685, 685, 5727, 3...   \n",
       "4440   [5, 8493, 586, 691, 328, 251, 623, 42, 397, 47...   \n",
       "8505   [1, 6, 31, 2024, 38146, 2181, 61, 76, 85, 1967...   \n",
       "18728  [79, 1708, 2, 774, 1139, 215, 14212, 44, 29434...   \n",
       "\n",
       "                                                  scores  target_test  \n",
       "23907  [0.641745388507843, 0.4675076901912689, 0.4003...            4  \n",
       "1629   [0.7249653339385986, 0.4582909345626831, 0.438...           10  \n",
       "4440   [0.6601946353912354, 0.6510505080223083, 0.572...           11  \n",
       "8505   [0.9602290391921997, 0.5080435276031494, 0.466...            4  \n",
       "18728  [0.8013430237770081, 0.688534677028656, 0.6055...           10  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_raw = pd.DataFrame()\n",
    "for i in os.listdir(\"./iteration_final/test_data_raw/\"):\n",
    "    if i.startswith('part'):\n",
    "        temp_df = pd.read_parquet(f\"./iteration_final/test_data_raw/{i}\")\n",
    "        test_data_raw = pd.concat([test_data_raw, temp_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>doc_type</th>\n",
       "      <th>paper_title</th>\n",
       "      <th>journal_name</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11688300</td>\n",
       "      <td>None</td>\n",
       "      <td>solving the traveling salesman problem in micr...</td>\n",
       "      <td>None</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>[microsoft excel, computer science, travelling...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13187899</td>\n",
       "      <td>Conference</td>\n",
       "      <td>expression recognition in videos using a weigh...</td>\n",
       "      <td>None</td>\n",
       "      <td>2011-05-01</td>\n",
       "      <td>[component, computer vision, computer science,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21335690</td>\n",
       "      <td>None</td>\n",
       "      <td>anisotropy of defect production in electron ir...</td>\n",
       "      <td>None</td>\n",
       "      <td>1968-10-31</td>\n",
       "      <td>[crystallography, electron, silicon, materials...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30574182</td>\n",
       "      <td>Patent</td>\n",
       "      <td>sprinkler trim ring</td>\n",
       "      <td>None</td>\n",
       "      <td>2000-02-03</td>\n",
       "      <td>[engineering, structural engineering, trim, ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47556196</td>\n",
       "      <td>Conference</td>\n",
       "      <td>the use of formal and informal models in objec...</td>\n",
       "      <td>None</td>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>[requirement, non functional requirement, soft...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paper_id    doc_type                                        paper_title  \\\n",
       "0  11688300        None  solving the traveling salesman problem in micr...   \n",
       "1  13187899  Conference  expression recognition in videos using a weigh...   \n",
       "2  21335690        None  anisotropy of defect production in electron ir...   \n",
       "3  30574182      Patent                                sprinkler trim ring   \n",
       "4  47556196  Conference  the use of formal and informal models in objec...   \n",
       "\n",
       "  journal_name publication_date  \\\n",
       "0         None       2010-01-01   \n",
       "1         None       2011-05-01   \n",
       "2         None       1968-10-31   \n",
       "3         None       2000-02-03   \n",
       "4         None       2001-01-01   \n",
       "\n",
       "                                              topics  \n",
       "0  [microsoft excel, computer science, travelling...  \n",
       "1  [component, computer vision, computer science,...  \n",
       "2  [crystallography, electron, silicon, materials...  \n",
       "3  [engineering, structural engineering, trim, ri...  \n",
       "4  [requirement, non functional requirement, soft...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_raw.to_parquet(\"test_raw.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting Everything Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(data, target_col, predict_col):\n",
    "    targets = tf.keras.preprocessing.sequence.pad_sequences(data[target_col].to_list(), maxlen=20, \n",
    "                                                            dtype='int64', padding='post', \n",
    "                                                            truncating='post', value=0)\n",
    "    targets = tf.RaggedTensor.from_tensor(targets, padding=0)\n",
    "    targets = encoding_layer(targets)\n",
    "    \n",
    "    predictions = tf.keras.preprocessing.sequence.pad_sequences(data[predict_col].to_list(), maxlen=30, \n",
    "                                                                dtype='int64', padding='post', \n",
    "                                                                truncating='post', value=0)\n",
    "    predictions = tf.RaggedTensor.from_tensor(predictions, padding=0)\n",
    "    predictions = encoding_layer(predictions)\n",
    "    \n",
    "    recall_score = tf.keras.metrics.Recall()\n",
    "    precision_score = tf.keras.metrics.Precision()\n",
    "    \n",
    "    recall_score.update_state(targets,predictions)\n",
    "    precision_score.update_state(targets,predictions)\n",
    "    \n",
    "    print(f\"Recall: {round(recall_score.result().numpy()*100, 1)}%\")\n",
    "    print(f\"Precision: {round(precision_score.result().numpy()*100, 1)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(test_data, 'target_tok', 'predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Sample of Targets vs Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_data = test_data.sample(10)\n",
    "for i,j in zip(sample_data['target_tok'].to_list(),sample_data['predictions'].to_list()):\n",
    "    print(\"Targets:\")\n",
    "    print([target_vocab_inv.get(x) for x in i])\n",
    "    print(\"\\n\")\n",
    "    print(\"Predictions:\")\n",
    "    print([target_vocab_inv.get(x) for x in j])\n",
    "    print(\"-----------------------------------------------------------------------------\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i,j in target_vocab_inv.items():\n",
    "    if i < 50:\n",
    "        print(f\"{i}: {j}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_ones = [2,3,4,5,6,7,11,12,13,14,16,18,19,21,24,29,32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get metrics for different levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to get df that contains the tag and the appropriate level (get only tags that show up in vocab)\n",
    "# also need to match up the tag with the tag_id from the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels_df = pd.read_parquet(\"./test_data_files/tag_levels.parquet\").fillna(6)\n",
    "levels_df['level'] = levels_df['level'].astype('int')\n",
    "# levels_df = pd.DataFrame(zip(level_ones, [1]*len(level_ones)), columns=['tag','level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels_df['level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_for_specific_level(old_df, levels, level_to_get=1):\n",
    "    df = old_df.copy()\n",
    "    tags_list = levels[levels['level']==level_to_get]['topic_name'].to_list()\n",
    "    tags_id_list = [target_vocab.get(x) for x in tags_list]\n",
    "    print(f\"Number of Level {level_to_get} Tags: {len(tags_list)}\")\n",
    "    \n",
    "    df[f'tags_level_{level_to_get}'] = df['target_tok'].apply(lambda x: [i for i in x if i in tags_id_list])\n",
    "    df[f'preds_level_{level_to_get}'] = df['predictions'].apply(lambda x: [i for i in x if i in tags_id_list])\n",
    "    \n",
    "    df[f'tag_level_{level_to_get}_len'] = df[f'tags_level_{level_to_get}'].apply(len)\n",
    "    df[f'pred_level_{level_to_get}_len'] = df[f'preds_level_{level_to_get}'].apply(len)\n",
    "    \n",
    "    papers_with_tag = (df[df[f'tag_level_{level_to_get}_len'] > 0].shape[0])/df.shape[0]\n",
    "    papers_with_pred = (df[df[f'pred_level_{level_to_get}_len'] > 0].shape[0])/df.shape[0]\n",
    "    \n",
    "    print(f\"Percentage of papers with Level {level_to_get} Tags: {round(papers_with_tag*100, 1)}\")\n",
    "    print(f\"Percentage of papers with Level {level_to_get} Preds: {round(papers_with_pred*100, 1)}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_to_check = 0\n",
    "df = get_df_for_specific_level(test_data, levels_df, level_to_check)\n",
    "get_metrics(df, f\"tags_level_{level_to_check}\", f\"preds_level_{level_to_check}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_to_check = 1\n",
    "df = get_df_for_specific_level(test_data, levels_df, level_to_check)\n",
    "get_metrics(df, f\"tags_level_{level_to_check}\", f\"preds_level_{level_to_check}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_to_check = 2\n",
    "df = get_df_for_specific_level(test_data, levels_df, level_to_check)\n",
    "get_metrics(df, f\"tags_level_{level_to_check}\", f\"preds_level_{level_to_check}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_to_check = 3\n",
    "df = get_df_for_specific_level(test_data, levels_df, level_to_check)\n",
    "get_metrics(df, f\"tags_level_{level_to_check}\", f\"preds_level_{level_to_check}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_to_check = 4\n",
    "df = get_df_for_specific_level(test_data, levels_df, level_to_check)\n",
    "get_metrics(df, f\"tags_level_{level_to_check}\", f\"preds_level_{level_to_check}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_to_check = 5\n",
    "df = get_df_for_specific_level(test_data, levels_df, level_to_check)\n",
    "get_metrics(df, f\"tags_level_{level_to_check}\", f\"preds_level_{level_to_check}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_to_check = 6\n",
    "df = get_df_for_specific_level(test_data, levels_df, level_to_check)\n",
    "get_metrics(df, f\"tags_level_{level_to_check}\", f\"preds_level_{level_to_check}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking into Data Available vs Not Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['journal'] = test_data['journal_tok'].apply(lambda x: [journal_vocab_inv.get(i) for i in x][0])\n",
    "test_data['doc_type'] = test_data['doc_type_tok'].apply(lambda x: [doc_vocab_inv.get(i) for i in x][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>doc_type_tok</th>\n",
       "      <th>journal_tok</th>\n",
       "      <th>target_tok</th>\n",
       "      <th>paper_title_tok</th>\n",
       "      <th>paper_title_mask</th>\n",
       "      <th>predictions</th>\n",
       "      <th>scores</th>\n",
       "      <th>journal</th>\n",
       "      <th>doc_type</th>\n",
       "      <th>paper_title_tok_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16657</th>\n",
       "      <td>2604922879</td>\n",
       "      <td>2001-11-28</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2, 2494, 5036, 43, 2373, 2110, 203, 14923, 10...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[4, 2, 157, 87, 78, 10, 28, 286, 167, 339, 48,...</td>\n",
       "      <td>[0.6048187017440796, 0.3559792637825012, 0.291...</td>\n",
       "      <td>[NONE]</td>\n",
       "      <td>Patent</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         paper_id publication_date doc_type_tok journal_tok target_tok  \\\n",
       "16657  2604922879       2001-11-28          [4]         [2]        [2]   \n",
       "\n",
       "                                         paper_title_tok  \\\n",
       "16657  [2, 2494, 5036, 43, 2373, 2110, 203, 14923, 10...   \n",
       "\n",
       "                                        paper_title_mask  \\\n",
       "16657  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                             predictions  \\\n",
       "16657  [4, 2, 157, 87, 78, 10, 28, 286, 167, 339, 48,...   \n",
       "\n",
       "                                                  scores journal doc_type  \\\n",
       "16657  [0.6048187017440796, 0.3559792637825012, 0.291...  [NONE]   Patent   \n",
       "\n",
       "       paper_title_tok_len  \n",
       "16657                   47  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Journal', 'Patent', '[NONE]', 'Conference', 'Repository', 'Thesis',\n",
       "       'Book', 'BookChapter'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['doc_type'].value_counts().index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Different Doc Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Journal\n",
      "Recall: 49.2%\n",
      "Precision: 27.9%\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Patent\n",
      "Recall: 48.3%\n",
      "Precision: 24.3%\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "[NONE]\n",
      "Recall: 51.5%\n",
      "Precision: 21.0%\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Conference\n",
      "Recall: 45.7%\n",
      "Precision: 28.6%\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Repository\n",
      "Recall: 45.7%\n",
      "Precision: 25.8%\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Thesis\n",
      "Recall: 52.6%\n",
      "Precision: 24.1%\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Book\n",
      "Recall: 57.6%\n",
      "Precision: 20.7%\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "BookChapter\n",
      "Recall: 68.4%\n",
      "Precision: 17.3%\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc_type in test_data['doc_type'].value_counts().index:\n",
    "    print(doc_type)\n",
    "    get_metrics(test_data[test_data['doc_type']==doc_type], \"target_tok\", \"predictions\")\n",
    "    print(\"-----------------------------------------------------------------------\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Journal vs No Journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking into Journal Title Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['paper_title_tok_len'] = test_data['paper_title_tok'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data['paper_title_tok_len'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 - 20, 20 - 40, 40+"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "mag_model_baseline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
