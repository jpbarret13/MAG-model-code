{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZJD011yhyWjD",
    "outputId": "35199005-d2bf-4120-bc5e-1e6fb8797d82"
   },
   "outputs": [],
   "source": [
    "# %pip install tensorflow==2.4.1\n",
    "# %pip install transformers\n",
    "# %pip install pyarrow\n",
    "# %pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oV5qIlEokph9"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import tensorflow_addons as tfa\n",
    "from math import ceil\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from transformers import RobertaTokenizer, RobertaTokenizerFast, TFRobertaModel, TFAlbertModel\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_iteration = 'iteration_final/funnel_transformer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9I2c4qh5FZa-"
   },
   "outputs": [],
   "source": [
    "with open(f\"./{model_iteration}/vocab/topics_vocab.pkl\", \"rb\") as f:\n",
    "    target_vocab = pickle.load(f)\n",
    "    \n",
    "target_vocab_inv = {j:i for i,j in target_vocab.items()}\n",
    "\n",
    "with open(f\"./{model_iteration}/vocab/doc_type_vocab.pkl\", \"rb\") as f:\n",
    "    doc_vocab = pickle.load(f)\n",
    "    \n",
    "doc_vocab_inv = {j:i for i,j in doc_vocab.items()}\n",
    "\n",
    "with open(f\"./{model_iteration}/vocab/journal_name_vocab.pkl\", \"rb\") as f:\n",
    "    journal_vocab = pickle.load(f)\n",
    "    \n",
    "journal_vocab_inv = {j:i for i,j in journal_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8dvSQiNsFHr4"
   },
   "outputs": [],
   "source": [
    "encoding_layer = tf.keras.layers.experimental.preprocessing.CategoryEncoding(\n",
    "    max_tokens=len(target_vocab)+1, output_mode=\"binary\", sparse=False)\n",
    "\n",
    "# # loss_fn = tf.keras.losses.CategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "# loss_fn = tfa.losses.SigmoidFocalCrossEntropy(alpha=0.25, gamma=2.0, \n",
    "#                                               reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "# metric_1 = tf.keras.metrics.CategoricalAccuracy()\n",
    "# metric_2 = tf.keras.metrics.Recall()\n",
    "# metric_3 = tf.keras.metrics.Precision()\n",
    "# metric_4 = tf.keras.metrics.TopKCategoricalAccuracy(K=10)\n",
    "# # Eventually will use with focal loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = pd.read_parquet(\"./iteration_1/tokenized_data/test/part-00000-tid-2414260718030245702-c767428a-156f-4065-8c31-91dc1cd77c46-6816-1-c000.snappy.parquet\").sample(3000)\n",
    "# test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-05 12:16:09.882619: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-11-05 12:16:09.882838: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "mag_model = tf.keras.models.load_model(f'./{model_iteration}/models/partial_model20211104_lr001_beta060_gamma10_nH4_nL1_thresh035/model_epoch09ckpt/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mag_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "paper_title_ids (InputLayer)    [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_embedding (Embedding)     (None, 512, 768)     23441664    paper_title_ids[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 512, 768)     0           title_embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/multiheadattention (M (None, 512, 768)     2362368     tf.__operators__.add[0][0]       \n",
      "                                                                 tf.__operators__.add[0][0]       \n",
      "                                                                 tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/att_dropout (Dropout) (None, 512, 768)     0           encoder_0/multiheadattention[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_1 (TFOpLam (None, 512, 768)     0           tf.__operators__.add[0][0]       \n",
      "                                                                 encoder_0/att_dropout[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/att_layernormalizatio (None, 512, 768)     1536        tf.__operators__.add_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/ffn (Sequential)      (None, 512, 768)     1574656     encoder_0/att_layernormalization[\n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/ffn_dropout (Dropout) (None, 512, 768)     0           encoder_0/ffn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_2 (TFOpLam (None, 512, 768)     0           encoder_0/att_layernormalization[\n",
      "                                                                 encoder_0/ffn_dropout[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/ffn_layernormalizatio (None, 512, 768)     1536        tf.__operators__.add_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512, 1024)    787456      encoder_0/ffn_layernormalization[\n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512, 1024)    0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "journal_id (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "doc_type_id (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "layer_norm_1 (LayerNormalizatio (None, 512, 1024)    2048        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "journal_embedding (Embedding)   (None, 1, 128)       5755392     journal_id[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "doc_type_embedding (Embedding)  (None, 1, 32)        320         doc_type_id[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "title_pooling_layer (GlobalAver (None, 1024)         0           layer_norm_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "journal_pooling_layer (GlobalAv (None, 128)          0           journal_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "doc_pooling_layer (GlobalAverag (None, 32)           0           doc_type_embedding[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (None, 1184)         0           title_pooling_layer[0][0]        \n",
      "                                                                 journal_pooling_layer[0][0]      \n",
      "                                                                 doc_pooling_layer[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1024)         1213440     tf.concat[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1024)         0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_norm_2 (LayerNormalizatio (None, 1024)         2048        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          262400      layer_norm_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 256)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_norm_3 (LayerNormalizatio (None, 256)          512         dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "cls (Dense)                     (None, 208751)       53649007    layer_norm_3[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 89,054,383\n",
      "Trainable params: 89,054,383\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mag_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = tf.keras.Model(inputs=mag_model.inputs, \n",
    "                             outputs=tf.math.top_k(mag_model.outputs, k=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "paper_title_ids (InputLayer)    [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_embedding (Embedding)     (None, 512, 768)     23441664    paper_title_ids[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 512, 768)     0           title_embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/multiheadattention (M (None, 512, 768)     2362368     tf.__operators__.add[0][0]       \n",
      "                                                                 tf.__operators__.add[0][0]       \n",
      "                                                                 tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/att_dropout (Dropout) (None, 512, 768)     0           encoder_0/multiheadattention[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_1 (TFOpLam (None, 512, 768)     0           tf.__operators__.add[0][0]       \n",
      "                                                                 encoder_0/att_dropout[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/att_layernormalizatio (None, 512, 768)     1536        tf.__operators__.add_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/ffn (Sequential)      (None, 512, 768)     1574656     encoder_0/att_layernormalization[\n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/ffn_dropout (Dropout) (None, 512, 768)     0           encoder_0/ffn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_2 (TFOpLam (None, 512, 768)     0           encoder_0/att_layernormalization[\n",
      "                                                                 encoder_0/ffn_dropout[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/ffn_layernormalizatio (None, 512, 768)     1536        tf.__operators__.add_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512, 1024)    787456      encoder_0/ffn_layernormalization[\n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512, 1024)    0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "journal_id (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "doc_type_id (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "layer_norm_1 (LayerNormalizatio (None, 512, 1024)    2048        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "journal_embedding (Embedding)   (None, 1, 128)       5755392     journal_id[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "doc_type_embedding (Embedding)  (None, 1, 32)        320         doc_type_id[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "title_pooling_layer (GlobalAver (None, 1024)         0           layer_norm_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "journal_pooling_layer (GlobalAv (None, 128)          0           journal_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "doc_pooling_layer (GlobalAverag (None, 32)           0           doc_type_embedding[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (None, 1184)         0           title_pooling_layer[0][0]        \n",
      "                                                                 journal_pooling_layer[0][0]      \n",
      "                                                                 doc_pooling_layer[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1024)         1213440     tf.concat[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1024)         0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_norm_2 (LayerNormalizatio (None, 1024)         2048        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          262400      layer_norm_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 256)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_norm_3 (LayerNormalizatio (None, 256)          512         dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "cls (Dense)                     (None, 208751)       53649007    layer_norm_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.top_k (TFOpLambda)      TopKV2(values=(1, No 0           cls[0][0]                        \n",
      "==================================================================================================\n",
      "Total params: 89,054,383\n",
      "Trainable params: 89,054,383\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_model_predictions(data_path):\n",
    "    # Get all of the files, load into single pandas dataframe\n",
    "    # split up into blocks of 3000 and get model output\n",
    "    file_names = [x for x in os.listdir(f\"./{model_iteration}/tokenized_data/test/\") if x.startswith('part')]\n",
    "    file_names.sort()\n",
    "    \n",
    "    full_df = pd.DataFrame()\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        temp_df = pd.read_parquet(f\"./{model_iteration}/tokenized_data/test/{file_name}\")\n",
    "        full_df = pd.concat([full_df, temp_df], axis=0)\n",
    "    \n",
    "    num_samples = 1000\n",
    "    preds_final = []\n",
    "    scores_final = []\n",
    "    for i in range(ceil(full_df.shape[0]/num_samples))[:1]:\n",
    "        print(i)\n",
    "        small_df = full_df.iloc[i*num_samples:(i+1)*num_samples, :].copy()\n",
    "        preds, scores = get_model_predictions(small_df)\n",
    "        preds_final += preds\n",
    "        scores_final += scores\n",
    "        \n",
    "    full_df = full_df.head(1000).copy()\n",
    "    \n",
    "    full_df['predictions'] = preds_final\n",
    "    full_df['scores'] = scores_final\n",
    "    \n",
    "    return full_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_predictions(input_data):\n",
    "    \n",
    "    paper_titles = tf.keras.preprocessing.sequence.pad_sequences(input_data['paper_title_tok'].to_list(), maxlen=512, \n",
    "                                                             dtype='int64', padding='post', \n",
    "                                                             truncating='post', value=0)\n",
    "    \n",
    "#     paper_titles_masks = tf.keras.preprocessing.sequence.pad_sequences(input_data['paper_title_mask'].to_list(), maxlen=512, \n",
    "#                                                                    dtype='int64', padding='post', \n",
    "#                                                                    truncating='post', value=0)\n",
    "    \n",
    "    doc_types = tf.convert_to_tensor(input_data['doc_type_tok'].to_list())\n",
    "    journal = tf.convert_to_tensor(input_data['journal_tok'].to_list())\n",
    "    \n",
    "    model_output = final_model([paper_titles, doc_types, journal])\n",
    "    \n",
    "    scores = model_output.values.numpy()[0][:,:20].tolist()\n",
    "    preds = model_output.indices.numpy()[0][:,:20].tolist()\n",
    "    \n",
    "    return preds, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_data = test_data.head(3).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_paper = tf.keras.preprocessing.sequence.pad_sequences(test_input_data['paper_title_tok'].to_list()[:1], maxlen=512, \n",
    "                                                             dtype='int64', padding='post', \n",
    "                                                             truncating='post', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc_types = tf.convert_to_tensor(test_input_data['doc_type_tok'].to_list()[:1])\n",
    "test_journal = tf.convert_to_tensor(test_input_data['journal_tok'].to_list()[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1,\n",
       "  27,\n",
       "  113,\n",
       "  437,\n",
       "  216,\n",
       "  4,\n",
       "  2252,\n",
       "  1155,\n",
       "  7,\n",
       "  567,\n",
       "  120,\n",
       "  152,\n",
       "  552,\n",
       "  30,\n",
       "  23,\n",
       "  8899,\n",
       "  789,\n",
       "  3,\n",
       "  5,\n",
       "  251]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model([test_paper, test_doc_types, test_journal]).indices.numpy()[0][:,:20].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "test_data = get_all_model_predictions(f\"./{model_iteration}/tokenized_data/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data.to_parquet(f\"./{model_iteration}/test_data/data_with_predictions.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_parquet(f\"./{model_iteration}/test_data/data_with_predictions.parquet\")\n",
    "test_data['target_test'] = test_data['target_tok'].apply(lambda x: [i for i in x if i!=-1])\n",
    "test_data['target_test'] = test_data['target_test'].apply(len)\n",
    "test_data = test_data[test_data['target_test'] > 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>doc_type_tok</th>\n",
       "      <th>journal_tok</th>\n",
       "      <th>target_tok</th>\n",
       "      <th>paper_title_tok</th>\n",
       "      <th>paper_title_mask</th>\n",
       "      <th>predictions</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9881621</td>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[46048, 105, 24]</td>\n",
       "      <td>[101, 27294, 8092, 1999, 2137, 15030, 5283, 102]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 27, 113, 437, 216, 4, 2252, 1155, 7, 567, ...</td>\n",
       "      <td>[0.4334312677383423, 0.26733309030532837, 0.22...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39454049</td>\n",
       "      <td>2011-06-01</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[18799, 175, 1777, 2486, 4, 113, 80]</td>\n",
       "      <td>[101, 2309, 2224, 12702, 10258, 21272, 2594, 5...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 437, 27, 36, 23, 22, 26, 1155, 4, 9, 120, ...</td>\n",
       "      <td>[0.4305480718612671, 0.3231326937675476, 0.266...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46834813</td>\n",
       "      <td>1974-10-01</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[199, 1065, 183, 21627, 7, 2161, 4441, 6444, 4...</td>\n",
       "      <td>[101, 1037, 2817, 1997, 1996, 1047, 2692, 1048...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 27, 113, 216, 4, 437, 2252, 1155, 7, 567, ...</td>\n",
       "      <td>[0.43141621351242065, 0.27147382497787476, 0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75469766</td>\n",
       "      <td>1974-01-01</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[13594, 293, 882, 945, 6887, 2049, 7, 10494]</td>\n",
       "      <td>[101, 1996, 2535, 1997, 14784, 1055, 27885, 13...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 27, 113, 216, 437, 4, 2252, 1155, 7, 567, ...</td>\n",
       "      <td>[0.4223536252975464, 0.2649972438812256, 0.234...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>129658366</td>\n",
       "      <td>2002-01-01</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[341, 18, 7125, 1298, 297, 16198, 1216, 3702, ...</td>\n",
       "      <td>[101, 1996, 9710, 1997, 4800, 2303, 11643, 200...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 27, 113, 216, 4, 437, 2252, 1155, 7, 567, ...</td>\n",
       "      <td>[0.43038231134414673, 0.2719365358352661, 0.23...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    paper_id publication_date doc_type_tok journal_tok  \\\n",
       "0    9881621       2001-01-01          [2]         [2]   \n",
       "1   39454049       2011-06-01          [4]         [2]   \n",
       "2   46834813       1974-10-01          [2]         [2]   \n",
       "3   75469766       1974-01-01          [2]         [2]   \n",
       "4  129658366       2002-01-01          [2]         [2]   \n",
       "\n",
       "                                          target_tok  \\\n",
       "0                                   [46048, 105, 24]   \n",
       "1               [18799, 175, 1777, 2486, 4, 113, 80]   \n",
       "2  [199, 1065, 183, 21627, 7, 2161, 4441, 6444, 4...   \n",
       "3       [13594, 293, 882, 945, 6887, 2049, 7, 10494]   \n",
       "4  [341, 18, 7125, 1298, 297, 16198, 1216, 3702, ...   \n",
       "\n",
       "                                     paper_title_tok  \\\n",
       "0   [101, 27294, 8092, 1999, 2137, 15030, 5283, 102]   \n",
       "1  [101, 2309, 2224, 12702, 10258, 21272, 2594, 5...   \n",
       "2  [101, 1037, 2817, 1997, 1996, 1047, 2692, 1048...   \n",
       "3  [101, 1996, 2535, 1997, 14784, 1055, 27885, 13...   \n",
       "4  [101, 1996, 9710, 1997, 4800, 2303, 11643, 200...   \n",
       "\n",
       "                                    paper_title_mask  \\\n",
       "0                           [1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                         predictions  \\\n",
       "0  [1, 27, 113, 437, 216, 4, 2252, 1155, 7, 567, ...   \n",
       "1  [1, 437, 27, 36, 23, 22, 26, 1155, 4, 9, 120, ...   \n",
       "2  [1, 27, 113, 216, 4, 437, 2252, 1155, 7, 567, ...   \n",
       "3  [1, 27, 113, 216, 437, 4, 2252, 1155, 7, 567, ...   \n",
       "4  [1, 27, 113, 216, 4, 437, 2252, 1155, 7, 567, ...   \n",
       "\n",
       "                                              scores  \n",
       "0  [0.4334312677383423, 0.26733309030532837, 0.22...  \n",
       "1  [0.4305480718612671, 0.3231326937675476, 0.266...  \n",
       "2  [0.43141621351242065, 0.27147382497787476, 0.2...  \n",
       "3  [0.4223536252975464, 0.2649972438812256, 0.234...  \n",
       "4  [0.43038231134414673, 0.2719365358352661, 0.23...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting Everything Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(data, target_col, predict_col):\n",
    "    targets = tf.keras.preprocessing.sequence.pad_sequences(data[target_col].to_list(), maxlen=20, \n",
    "                                                            dtype='int64', padding='post', \n",
    "                                                            truncating='post', value=0)\n",
    "    targets = tf.RaggedTensor.from_tensor(targets, padding=0)\n",
    "    targets = encoding_layer(targets)\n",
    "    \n",
    "    predictions = tf.keras.preprocessing.sequence.pad_sequences(data[predict_col].to_list(), maxlen=30, \n",
    "                                                                dtype='int64', padding='post', \n",
    "                                                                truncating='post', value=0)\n",
    "    predictions = tf.RaggedTensor.from_tensor(predictions, padding=0)\n",
    "    predictions = encoding_layer(predictions)\n",
    "    \n",
    "    recall_score = tf.keras.metrics.Recall()\n",
    "    precision_score = tf.keras.metrics.Precision()\n",
    "    \n",
    "    recall_score.update_state(targets,predictions)\n",
    "    precision_score.update_state(targets,predictions)\n",
    "    \n",
    "    print(f\"Recall: {round(recall_score.result().numpy()*100, 1)}%\")\n",
    "    print(f\"Precision: {round(precision_score.result().numpy()*100, 1)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(test_data, 'target_tok', 'predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Sample of Targets vs Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_data = test_data.sample(10)\n",
    "for i,j in zip(sample_data['target_tok'].to_list(),sample_data['predictions'].to_list()):\n",
    "    print(\"Targets:\")\n",
    "    print([target_vocab_inv.get(x) for x in i])\n",
    "    print(\"\\n\")\n",
    "    print(\"Predictions:\")\n",
    "    print([target_vocab_inv.get(x) for x in j])\n",
    "    print(\"-----------------------------------------------------------------------------\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i,j in target_vocab_inv.items():\n",
    "    if i < 50:\n",
    "        print(f\"{i}: {j}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_ones = [2,3,4,5,6,7,11,12,13,14,16,18,19,21,24,29,32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get metrics for different levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to get df that contains the tag and the appropriate level (get only tags that show up in vocab)\n",
    "# also need to match up the tag with the tag_id from the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels_df = pd.read_parquet(\"./test_data_files/tag_levels.parquet\").fillna(6)\n",
    "levels_df['level'] = levels_df['level'].astype('int')\n",
    "# levels_df = pd.DataFrame(zip(level_ones, [1]*len(level_ones)), columns=['tag','level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels_df['level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_for_specific_level(old_df, levels, level_to_get=1):\n",
    "    df = old_df.copy()\n",
    "    tags_list = levels[levels['level']==level_to_get]['topic_name'].to_list()\n",
    "    tags_id_list = [target_vocab.get(x) for x in tags_list]\n",
    "    print(f\"Number of Level {level_to_get} Tags: {len(tags_list)}\")\n",
    "    \n",
    "    df[f'tags_level_{level_to_get}'] = df['target_tok'].apply(lambda x: [i for i in x if i in tags_id_list])\n",
    "    df[f'preds_level_{level_to_get}'] = df['predictions'].apply(lambda x: [i for i in x if i in tags_id_list])\n",
    "    \n",
    "    df[f'tag_level_{level_to_get}_len'] = df[f'tags_level_{level_to_get}'].apply(len)\n",
    "    df[f'pred_level_{level_to_get}_len'] = df[f'preds_level_{level_to_get}'].apply(len)\n",
    "    \n",
    "    papers_with_tag = (df[df[f'tag_level_{level_to_get}_len'] > 0].shape[0])/df.shape[0]\n",
    "    papers_with_pred = (df[df[f'pred_level_{level_to_get}_len'] > 0].shape[0])/df.shape[0]\n",
    "    \n",
    "    print(f\"Percentage of papers with Level {level_to_get} Tags: {round(papers_with_tag*100, 1)}\")\n",
    "    print(f\"Percentage of papers with Level {level_to_get} Preds: {round(papers_with_pred*100, 1)}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_to_check = 0\n",
    "df = get_df_for_specific_level(test_data, levels_df, level_to_check)\n",
    "get_metrics(df, f\"tags_level_{level_to_check}\", f\"preds_level_{level_to_check}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_to_check = 1\n",
    "df = get_df_for_specific_level(test_data, levels_df, level_to_check)\n",
    "get_metrics(df, f\"tags_level_{level_to_check}\", f\"preds_level_{level_to_check}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_to_check = 2\n",
    "df = get_df_for_specific_level(test_data, levels_df, level_to_check)\n",
    "get_metrics(df, f\"tags_level_{level_to_check}\", f\"preds_level_{level_to_check}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_to_check = 3\n",
    "df = get_df_for_specific_level(test_data, levels_df, level_to_check)\n",
    "get_metrics(df, f\"tags_level_{level_to_check}\", f\"preds_level_{level_to_check}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_to_check = 4\n",
    "df = get_df_for_specific_level(test_data, levels_df, level_to_check)\n",
    "get_metrics(df, f\"tags_level_{level_to_check}\", f\"preds_level_{level_to_check}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_to_check = 5\n",
    "df = get_df_for_specific_level(test_data, levels_df, level_to_check)\n",
    "get_metrics(df, f\"tags_level_{level_to_check}\", f\"preds_level_{level_to_check}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_to_check = 6\n",
    "df = get_df_for_specific_level(test_data, levels_df, level_to_check)\n",
    "get_metrics(df, f\"tags_level_{level_to_check}\", f\"preds_level_{level_to_check}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking into Data Available vs Not Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['journal'] = test_data['journal_tok'].apply(lambda x: [journal_vocab_inv.get(i) for i in x][0])\n",
    "test_data['doc_type'] = test_data['doc_type_tok'].apply(lambda x: [doc_vocab_inv.get(i) for i in x][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>doc_type_tok</th>\n",
       "      <th>journal_tok</th>\n",
       "      <th>target_tok</th>\n",
       "      <th>paper_title_tok</th>\n",
       "      <th>paper_title_mask</th>\n",
       "      <th>predictions</th>\n",
       "      <th>scores</th>\n",
       "      <th>journal</th>\n",
       "      <th>doc_type</th>\n",
       "      <th>paper_title_tok_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16657</th>\n",
       "      <td>2604922879</td>\n",
       "      <td>2001-11-28</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2, 2494, 5036, 43, 2373, 2110, 203, 14923, 10...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[4, 2, 157, 87, 78, 10, 28, 286, 167, 339, 48,...</td>\n",
       "      <td>[0.6048187017440796, 0.3559792637825012, 0.291...</td>\n",
       "      <td>[NONE]</td>\n",
       "      <td>Patent</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         paper_id publication_date doc_type_tok journal_tok target_tok  \\\n",
       "16657  2604922879       2001-11-28          [4]         [2]        [2]   \n",
       "\n",
       "                                         paper_title_tok  \\\n",
       "16657  [2, 2494, 5036, 43, 2373, 2110, 203, 14923, 10...   \n",
       "\n",
       "                                        paper_title_mask  \\\n",
       "16657  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                             predictions  \\\n",
       "16657  [4, 2, 157, 87, 78, 10, 28, 286, 167, 339, 48,...   \n",
       "\n",
       "                                                  scores journal doc_type  \\\n",
       "16657  [0.6048187017440796, 0.3559792637825012, 0.291...  [NONE]   Patent   \n",
       "\n",
       "       paper_title_tok_len  \n",
       "16657                   47  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Journal', 'Patent', '[NONE]', 'Conference', 'Repository', 'Thesis',\n",
       "       'Book', 'BookChapter'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['doc_type'].value_counts().index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Different Doc Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Journal\n",
      "Recall: 49.2%\n",
      "Precision: 27.9%\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Patent\n",
      "Recall: 48.3%\n",
      "Precision: 24.3%\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "[NONE]\n",
      "Recall: 51.5%\n",
      "Precision: 21.0%\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Conference\n",
      "Recall: 45.7%\n",
      "Precision: 28.6%\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Repository\n",
      "Recall: 45.7%\n",
      "Precision: 25.8%\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Thesis\n",
      "Recall: 52.6%\n",
      "Precision: 24.1%\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Book\n",
      "Recall: 57.6%\n",
      "Precision: 20.7%\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "BookChapter\n",
      "Recall: 68.4%\n",
      "Precision: 17.3%\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc_type in test_data['doc_type'].value_counts().index:\n",
    "    print(doc_type)\n",
    "    get_metrics(test_data[test_data['doc_type']==doc_type], \"target_tok\", \"predictions\")\n",
    "    print(\"-----------------------------------------------------------------------\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Journal vs No Journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking into Journal Title Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['paper_title_tok_len'] = test_data['paper_title_tok'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data['paper_title_tok_len'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 - 20, 20 - 40, 40+"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "mag_model_baseline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
