{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "metric-secretariat",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.4.1\n",
    "# !pip install transformers\n",
    "# !pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "latin-japanese",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-14 03:19:11.927472: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-14 03:19:11.927500: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4a900db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_targets(targs):\n",
    "    if targs[0] == -1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b60fe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_length(data, seq_len=512):\n",
    "    return data[:seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "charged-familiar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfrecords_dataset(data, iter_num, dataset_type='train'):\n",
    "#     paper_title = tf.keras.preprocessing.sequence.pad_sequences(data['paper_title_tok'].to_list(), \n",
    "#                                                                 maxlen=64, dtype='int64', \n",
    "#                                                                 padding='post', truncating='post', value=0)\n",
    "    \n",
    "#     abstract = tf.keras.preprocessing.sequence.pad_sequences(data['abstract_tok'].to_list(), \n",
    "#                                                              maxlen=512, dtype='int64', \n",
    "#                                                              padding='post', truncating='post', value=0)\n",
    "    \n",
    "    data['no_target'] = data['target_tok'].apply(check_targets)\n",
    "    data = data[data['no_target']==0].copy()\n",
    "    \n",
    "    data['paper_title_tok'] = data['paper_title_tok'].apply(cut_length, args=(64,))\n",
    "    data['abstract_tok'] = data['abstract_tok'].apply(cut_length, args=(256,))\n",
    "    \n",
    "    paper_title = tf.ragged.constant(data['paper_title_tok'].to_list())\n",
    "    abstract = tf.ragged.constant(data['abstract_tok'].to_list())\n",
    "    \n",
    "    targets = tf.keras.preprocessing.sequence.pad_sequences(data['target_tok'].to_list(), maxlen=20, \n",
    "                                                            dtype='int64', padding='post', \n",
    "                                                            truncating='post', value=0)\n",
    "\n",
    "    ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(paper_title),\n",
    "                              tf.data.Dataset.from_tensor_slices(abstract),\n",
    "                              tf.data.Dataset.from_tensor_slices(data['journal_tok'].to_list()),\n",
    "                              tf.data.Dataset.from_tensor_slices(data['doc_type_tok'].to_list()),\n",
    "                              tf.data.Dataset.from_tensor_slices(targets)))\n",
    "    \n",
    "    serialized_features_dataset = ds.map(tf_serialize_example)\n",
    "    \n",
    "    filename = f\"./data/iteration_1/tfrecords/{dataset_type}/{str(iter_num).zfill(4)}.tfrecord\"\n",
    "    writer = tf.data.experimental.TFRecordWriter(filename)\n",
    "    writer.write(serialized_features_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "portable-military",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_serialize_example(f0, f1, f2, f3, f4):\n",
    "    tf_string = tf.py_function(serialize_example, (f0, f1, f2, f3, f4), tf.string)\n",
    "    return tf.reshape(tf_string, ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "interior-vancouver",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(paper_title, abstract, journal, doc_type, targets):\n",
    "    paper_title_list = tf.train.Int64List(value=paper_title.numpy().tolist())\n",
    "    abstract_list = tf.train.Int64List(value=abstract.numpy().tolist())\n",
    "    journal_list = tf.train.Int64List(value=journal.numpy().tolist())\n",
    "    doc_type_list = tf.train.Int64List(value=doc_type.numpy().tolist())\n",
    "    targets_list = tf.train.Int64List(value=targets.numpy().tolist())\n",
    "    \n",
    "    paper_title_feature = tf.train.Feature(int64_list = paper_title_list)\n",
    "    abstract_feature = tf.train.Feature(int64_list = abstract_list)\n",
    "    journal_feature = tf.train.Feature(int64_list = journal_list)\n",
    "    doc_type_feature = tf.train.Feature(int64_list = doc_type_list)\n",
    "    targets_feature = tf.train.Feature(int64_list = targets_list)\n",
    "    \n",
    "    features_for_example = {\n",
    "        'paper_title': paper_title_feature,\n",
    "        'abstract': abstract_feature,\n",
    "        'journal': journal_feature,\n",
    "        'doc_type': doc_type_feature,\n",
    "        'targets': targets_feature\n",
    "    }\n",
    "    \n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=features_for_example))\n",
    "    \n",
    "    return example_proto.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "approved-spoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_part_file_into_tfrecord(base_path, dataset_type='train'):\n",
    "    file_list = [x for x in os.listdir(f\"{base_path}{dataset_type}\") if x.endswith('parquet')]\n",
    "    file_list.sort()\n",
    "    print(f\"There are {len(file_list)} files for {dataset_type}\")\n",
    "    for i, file_name in enumerate(file_list):\n",
    "        data = pd.read_parquet(f\"{base_path}{dataset_type}/{file_name}\")\n",
    "        print(f\"_____File number: {i} ({data.shape[0]} samples)\")\n",
    "        create_tfrecords_dataset(data, i, dataset_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "veterinary-alfred",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_file_path = f\"./data/iteration_1/tokenized_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99310f8a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 100 files for train\n",
      "_____File number: 0 (1486621 samples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-14 03:30:03.591771: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-01-14 03:30:03.594121: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-01-14 03:30:03.594129: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-01-14 03:30:03.594143: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-31-31-169.ec2.internal): /proc/driver/nvidia/version does not exist\n",
      "2022-01-14 03:30:03.595282: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-14 03:30:03.596171: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-01-14 03:30:03.599711: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 73200376 exceeds 10% of free system memory.\n",
      "2022-01-14 03:33:17.035662: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 620239392 exceeds 10% of free system memory.\n",
      "2022-01-14 03:33:34.688094: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 95128832 exceeds 10% of free system memory.\n",
      "2022-01-14 03:33:35.525303: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 95128832 exceeds 10% of free system memory.\n",
      "2022-01-14 03:34:05.860365: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 237822080 exceeds 10% of free system memory.\n",
      "2022-01-14 03:34:06.022363: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-01-14 03:34:06.050342: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2899985000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____File number: 1 (991672 samples)\n",
      "_____File number: 2 (1486468 samples)\n",
      "_____File number: 3 (1486876 samples)\n",
      "_____File number: 4 (1487474 samples)\n",
      "_____File number: 5 (990006 samples)\n",
      "_____File number: 6 (1487822 samples)\n",
      "_____File number: 7 (991953 samples)\n",
      "_____File number: 8 (993361 samples)\n",
      "_____File number: 9 (991922 samples)\n",
      "_____File number: 10 (991206 samples)\n",
      "_____File number: 11 (992267 samples)\n",
      "_____File number: 12 (1486685 samples)\n",
      "_____File number: 13 (991641 samples)\n",
      "_____File number: 14 (992454 samples)\n",
      "_____File number: 15 (991873 samples)\n",
      "_____File number: 16 (992159 samples)\n",
      "_____File number: 17 (1488779 samples)\n",
      "_____File number: 18 (989652 samples)\n",
      "_____File number: 19 (989146 samples)\n",
      "_____File number: 20 (1486070 samples)\n",
      "_____File number: 21 (991605 samples)\n",
      "_____File number: 22 (991345 samples)\n",
      "_____File number: 23 (990142 samples)\n",
      "_____File number: 24 (992845 samples)\n",
      "_____File number: 25 (1487145 samples)\n",
      "_____File number: 26 (990507 samples)\n",
      "_____File number: 27 (988171 samples)\n",
      "_____File number: 28 (992142 samples)\n",
      "_____File number: 29 (990615 samples)\n",
      "_____File number: 30 (991654 samples)\n",
      "_____File number: 31 (990141 samples)\n",
      "_____File number: 32 (991296 samples)\n",
      "_____File number: 33 (992830 samples)\n",
      "_____File number: 35 (990677 samples)\n",
      "_____File number: 36 (990362 samples)\n",
      "_____File number: 37 (990400 samples)\n",
      "_____File number: 38 (991297 samples)\n",
      "_____File number: 39 (992643 samples)\n",
      "_____File number: 40 (988356 samples)\n",
      "_____File number: 41 (991997 samples)\n",
      "_____File number: 42 (990650 samples)\n",
      "_____File number: 43 (992171 samples)\n",
      "_____File number: 44 (990342 samples)\n",
      "_____File number: 45 (991129 samples)\n",
      "_____File number: 46 (991995 samples)\n",
      "_____File number: 47 (990677 samples)\n",
      "_____File number: 48 (990243 samples)\n",
      "_____File number: 49 (990365 samples)\n",
      "_____File number: 51 (992844 samples)\n",
      "_____File number: 52 (991824 samples)\n",
      "_____File number: 53 (991649 samples)\n",
      "_____File number: 54 (992769 samples)\n",
      "_____File number: 55 (992950 samples)\n",
      "_____File number: 56 (992885 samples)\n",
      "_____File number: 57 (991494 samples)\n",
      "_____File number: 58 (991276 samples)\n",
      "_____File number: 59 (991659 samples)\n",
      "_____File number: 60 (495929 samples)\n",
      "_____File number: 61 (991300 samples)\n",
      "_____File number: 62 (992874 samples)\n",
      "_____File number: 63 (990967 samples)\n",
      "_____File number: 64 (991234 samples)\n",
      "_____File number: 65 (991276 samples)\n",
      "_____File number: 66 (992813 samples)\n",
      "_____File number: 67 (992076 samples)\n",
      "_____File number: 68 (989885 samples)\n",
      "_____File number: 69 (989034 samples)\n",
      "_____File number: 70 (494974 samples)\n",
      "_____File number: 72 (990521 samples)\n",
      "_____File number: 73 (992266 samples)\n",
      "_____File number: 74 (993605 samples)\n",
      "_____File number: 75 (990063 samples)\n",
      "_____File number: 76 (991938 samples)\n",
      "_____File number: 77 (990092 samples)\n",
      "_____File number: 78 (990717 samples)\n",
      "_____File number: 79 (991243 samples)\n",
      "_____File number: 80 (992149 samples)\n",
      "_____File number: 81 (991181 samples)\n",
      "_____File number: 82 (495288 samples)\n",
      "_____File number: 83 (992481 samples)\n",
      "_____File number: 84 (495869 samples)\n",
      "_____File number: 85 (991036 samples)\n",
      "_____File number: 86 (991484 samples)\n",
      "_____File number: 87 (495719 samples)\n",
      "_____File number: 88 (992689 samples)\n",
      "_____File number: 89 (990684 samples)\n",
      "_____File number: 90 (496333 samples)\n",
      "_____File number: 91 (990569 samples)\n",
      "_____File number: 92 (495268 samples)\n",
      "_____File number: 93 (991454 samples)\n",
      "_____File number: 94 (991387 samples)\n",
      "_____File number: 95 (989834 samples)\n",
      "_____File number: 96 (495722 samples)\n",
      "_____File number: 97 (990274 samples)\n",
      "_____File number: 98 (495557 samples)\n",
      "_____File number: 99 (990158 samples)\n",
      "CPU times: user 11h 24min 53s, sys: 31min 29s, total: 11h 56min 22s\n",
      "Wall time: 10h 38min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "turn_part_file_into_tfrecord(base_file_path, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8fcfb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10 files for val\n",
      "_____File number: 0 (62547 samples)\n",
      "_____File number: 1 (24745 samples)\n",
      "_____File number: 2 (69705 samples)\n",
      "_____File number: 3 (60810 samples)\n",
      "_____File number: 4 (26665 samples)\n",
      "_____File number: 5 (69207 samples)\n",
      "_____File number: 6 (24754 samples)\n",
      "_____File number: 7 (26758 samples)\n",
      "_____File number: 8 (60213 samples)\n",
      "_____File number: 9 (22548 samples)\n",
      "CPU times: user 3min 6s, sys: 8.24 s, total: 3min 14s\n",
      "Wall time: 2min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "turn_part_file_into_tfrecord(base_file_path, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9aefdeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.77 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# turn_part_file_into_tfrecord(base_file_path, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-exchange",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-syntax",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
