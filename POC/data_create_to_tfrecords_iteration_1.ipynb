{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "metric-secretariat",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.4.1\n",
    "# !pip install transformers\n",
    "# !pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "latin-japanese",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from math import ceil\n",
    "from transformers import AlbertTokenizerFast, TFAlbertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4a900db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_targets(targs):\n",
    "    if targs[0] == -1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "charged-familiar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfrecords_dataset(data, iter_num, dataset_type='train'):\n",
    "#     paper_title = tf.keras.preprocessing.sequence.pad_sequences(data['paper_title_tok'].to_list(), \n",
    "#                                                                 maxlen=512, dtype='int64', \n",
    "#                                                                 padding='post', truncating='post', value=0)\n",
    "    data['no_target'] = data['target_tok'].apply(check_targets)\n",
    "    data = data[data['no_target']==0].copy()\n",
    "    \n",
    "    paper_title = tf.ragged.constant(data['paper_title_tok'].to_list())\n",
    "    \n",
    "    paper_mask = tf.ragged.constant(data['paper_title_mask'].to_list())\n",
    "    \n",
    "    targets = tf.keras.preprocessing.sequence.pad_sequences(data['target_tok'].to_list(), maxlen=20, \n",
    "                                                            dtype='int64', padding='post', \n",
    "                                                            truncating='post', value=0)\n",
    "\n",
    "    ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(paper_title),\n",
    "                              tf.data.Dataset.from_tensor_slices(paper_mask),\n",
    "                              tf.data.Dataset.from_tensor_slices(data['journal_tok'].to_list()),\n",
    "                              tf.data.Dataset.from_tensor_slices(data['doc_type_tok'].to_list()),\n",
    "                              tf.data.Dataset.from_tensor_slices(targets)))\n",
    "    \n",
    "    serialized_features_dataset = ds.map(tf_serialize_example)\n",
    "    \n",
    "    filename = f\"./iteration_1/tfrecords/{dataset_type}/{str(iter_num).zfill(4)}.tfrecord\"\n",
    "    writer = tf.data.experimental.TFRecordWriter(filename)\n",
    "    writer.write(serialized_features_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "portable-military",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_serialize_example(f0, f1, f2, f3, f4):\n",
    "    tf_string = tf.py_function(serialize_example, (f0, f1, f2, f3, f4), tf.string)\n",
    "    return tf.reshape(tf_string, ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "interior-vancouver",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(paper_title, paper_mask, journal, doc_type, targets):\n",
    "    paper_title_list = tf.train.Int64List(value=paper_title.numpy().tolist())\n",
    "    paper_mask_list = tf.train.Int64List(value=paper_mask.numpy().tolist())\n",
    "    journal_list = tf.train.Int64List(value=journal.numpy().tolist())\n",
    "    doc_type_list = tf.train.Int64List(value=doc_type.numpy().tolist())\n",
    "    targets_list = tf.train.Int64List(value=targets.numpy().tolist())\n",
    "    \n",
    "    paper_title_feature = tf.train.Feature(int64_list = paper_title_list)\n",
    "    paper_mask_feature = tf.train.Feature(int64_list = paper_mask_list)\n",
    "    journal_feature = tf.train.Feature(int64_list = journal_list)\n",
    "    doc_type_feature = tf.train.Feature(int64_list = doc_type_list)\n",
    "    targets_feature = tf.train.Feature(int64_list = targets_list)\n",
    "    \n",
    "    features_for_example = {\n",
    "        'paper_title': paper_title_feature,\n",
    "        'paper_mask': paper_mask_feature,\n",
    "        'journal': journal_feature,\n",
    "        'doc_type': doc_type_feature,\n",
    "        'targets': targets_feature\n",
    "    }\n",
    "    \n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=features_for_example))\n",
    "    \n",
    "    return example_proto.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "approved-spoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_part_file_into_tfrecord(base_path, dataset_type='train'):\n",
    "    file_list = [x for x in os.listdir(f\"{base_path}{dataset_type}\") if x.endswith('parquet')]\n",
    "    file_list.sort()\n",
    "    print(f\"There are {len(file_list)} files for {dataset_type}\")\n",
    "    for i, file_name in enumerate(file_list):\n",
    "        data = pd.read_parquet(f\"{base_path}{dataset_type}/{file_name}\")\n",
    "        print(f\"_____File number: {i} ({data.shape[0]} samples)\")\n",
    "        create_tfrecords_dataset(data, i, dataset_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "veterinary-alfred",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_file_path = f\"./iteration_1/tokenized_data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604b7505",
   "metadata": {},
   "source": [
    "#### Without padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99310f8a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 50 files for train\n",
      "_____File number: 0 (3895117 samples)\n",
      "_____File number: 1 (5368215 samples)\n",
      "_____File number: 2 (2782313 samples)\n",
      "_____File number: 3 (3894716 samples)\n",
      "_____File number: 4 (3456738 samples)\n",
      "_____File number: 5 (5367319 samples)\n",
      "_____File number: 6 (3143801 samples)\n",
      "_____File number: 7 (3893588 samples)\n",
      "_____File number: 8 (5366319 samples)\n",
      "_____File number: 9 (5366313 samples)\n",
      "_____File number: 10 (5364301 samples)\n",
      "_____File number: 11 (5358874 samples)\n",
      "_____File number: 12 (3894438 samples)\n",
      "_____File number: 13 (5357879 samples)\n",
      "_____File number: 14 (4450546 samples)\n",
      "_____File number: 15 (4450080 samples)\n",
      "_____File number: 16 (3136976 samples)\n",
      "_____File number: 17 (2225998 samples)\n",
      "_____File number: 18 (4449857 samples)\n",
      "_____File number: 19 (4450181 samples)\n",
      "_____File number: 20 (4452179 samples)\n",
      "_____File number: 21 (4259843 samples)\n",
      "_____File number: 22 (4451241 samples)\n",
      "_____File number: 23 (4251282 samples)\n",
      "_____File number: 24 (4252101 samples)\n",
      "_____File number: 25 (4250647 samples)\n",
      "_____File number: 26 (4811477 samples)\n",
      "_____File number: 27 (2226494 samples)\n",
      "_____File number: 28 (4451241 samples)\n",
      "_____File number: 29 (4248948 samples)\n",
      "_____File number: 30 (3339270 samples)\n",
      "_____File number: 31 (4450946 samples)\n",
      "_____File number: 32 (4448551 samples)\n",
      "_____File number: 33 (4449643 samples)\n",
      "_____File number: 34 (2224889 samples)\n",
      "_____File number: 35 (3337711 samples)\n",
      "_____File number: 36 (2225652 samples)\n",
      "_____File number: 37 (4452215 samples)\n",
      "_____File number: 38 (2225730 samples)\n",
      "_____File number: 39 (2225729 samples)\n",
      "_____File number: 40 (2225736 samples)\n",
      "_____File number: 41 (4815148 samples)\n",
      "_____File number: 42 (3338395 samples)\n",
      "_____File number: 43 (4811992 samples)\n",
      "_____File number: 44 (4247388 samples)\n",
      "_____File number: 45 (4246914 samples)\n",
      "_____File number: 46 (4244511 samples)\n",
      "_____File number: 47 (3338024 samples)\n",
      "_____File number: 48 (2225368 samples)\n",
      "_____File number: 49 (3337691 samples)\n",
      "CPU times: user 21h 52min 44s, sys: 1h 20min 9s, total: 23h 12min 54s\n",
      "Wall time: 2d 14min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "turn_part_file_into_tfrecord(base_file_path, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8fcfb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10 files for val\n",
      "_____File number: 0 (62541 samples)\n",
      "_____File number: 1 (121255 samples)\n",
      "_____File number: 2 (120625 samples)\n",
      "_____File number: 3 (119421 samples)\n",
      "_____File number: 4 (95079 samples)\n",
      "_____File number: 5 (61018 samples)\n",
      "_____File number: 6 (103451 samples)\n",
      "_____File number: 7 (59970 samples)\n",
      "_____File number: 8 (90211 samples)\n",
      "_____File number: 9 (59167 samples)\n",
      "CPU times: user 5min 51s, sys: 20.1 s, total: 6min 11s\n",
      "Wall time: 5min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "turn_part_file_into_tfrecord(base_file_path, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9aefdeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5 files for test\n",
      "_____File number: 0 (38174 samples)\n",
      "_____File number: 1 (27355 samples)\n",
      "_____File number: 2 (29110 samples)\n",
      "_____File number: 3 (24559 samples)\n",
      "_____File number: 4 (31432 samples)\n",
      "CPU times: user 1min, sys: 3.35 s, total: 1min 3s\n",
      "Wall time: 54.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "turn_part_file_into_tfrecord(base_file_path, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-exchange",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-syntax",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
