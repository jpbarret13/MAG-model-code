{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZJD011yhyWjD",
    "outputId": "35199005-d2bf-4120-bc5e-1e6fb8797d82"
   },
   "outputs": [],
   "source": [
    "# %pip install tensorflow==2.4.1\n",
    "# %pip install transformers\n",
    "# %pip install pyarrow\n",
    "# %pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oV5qIlEokph9"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import tensorflow_addons as tfa\n",
    "from transformers import RobertaTokenizer, RobertaTokenizerFast, TFRobertaModel, TFAlbertModel\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_iteration = 'iteration_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-15 12:44:09.648350: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "9I2c4qh5FZa-"
   },
   "outputs": [],
   "source": [
    "with open(f\"./{model_iteration}/vocab/topics_vocab.pkl\", \"rb\") as f:\n",
    "    target_vocab = pickle.load(f)\n",
    "    \n",
    "with open(f\"./{model_iteration}/vocab/doc_type_vocab.pkl\", \"rb\") as f:\n",
    "    doc_vocab = pickle.load(f)\n",
    "    \n",
    "with open(f\"./{model_iteration}/vocab/journal_name_vocab.pkl\", \"rb\") as f:\n",
    "    journal_vocab = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "8dvSQiNsFHr4"
   },
   "outputs": [],
   "source": [
    "encoding_layer = tf.keras.layers.experimental.preprocessing.CategoryEncoding(\n",
    "    max_tokens=len(target_vocab)+1, output_mode=\"binary\", sparse=False)\n",
    "\n",
    "# loss_fn = tf.keras.losses.CategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "loss_fn = tfa.losses.SigmoidFocalCrossEntropy(alpha=0.25, gamma=2.0, \n",
    "                                              reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "metric_1 = tf.keras.metrics.CategoricalAccuracy()\n",
    "metric_2 = tf.keras.metrics.Recall()\n",
    "metric_3 = tf.keras.metrics.Precision()\n",
    "metric_4 = tf.keras.metrics.TopKCategoricalAccuracy(K=10)\n",
    "# Eventually will use with focal loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "ZQHm5JWGEYb0"
   },
   "outputs": [],
   "source": [
    "class CustomModel(tf.keras.Model):\n",
    "    def train_step(self, inputs):\n",
    "        old_features, labels = inputs\n",
    "        labels = tf.RaggedTensor.from_tensor(labels, padding=0)\n",
    "        paper_titles = old_features[0][:,:512].to_tensor(shape=[None, 512])\n",
    "        paper_masks = old_features[1][:,:512].to_tensor(shape=[None, 512])\n",
    "        \n",
    "        features = (paper_titles, paper_masks, old_features[2], old_features[3])\n",
    "        labels = encoding_layer(labels)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(features, training=True)\n",
    "            loss = loss_fn(labels, predictions)\n",
    "\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        metric_1.update_state(labels, predictions)\n",
    "        metric_2.update_state(labels, predictions)\n",
    "        metric_3.update_state(labels, predictions)\n",
    "        metric_4.update_state(labels, predictions)\n",
    "\n",
    "        return {\"loss\": loss, \n",
    "                \"accuracy\": metric_1.result(), \n",
    "                \"recall\": metric_2.result(), \n",
    "                \"precision\": metric_3.result(), \n",
    "                \"topK15\": metric_4.result()}\n",
    "  \n",
    "    def test_step(self, inputs):\n",
    "        old_features, labels = inputs\n",
    "        labels = tf.RaggedTensor.from_tensor(labels, padding=0)\n",
    "        paper_titles = old_features[0][:,:512].to_tensor(shape=[None, 512])\n",
    "        paper_masks = old_features[1][:,:512].to_tensor(shape=[None, 512])\n",
    "        \n",
    "        features = (paper_titles, paper_masks, old_features[2], old_features[3])\n",
    "        labels = encoding_layer(labels)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(features, training=False)\n",
    "            loss = loss_fn(labels, predictions)\n",
    "\n",
    "        metric_1.update_state(labels, predictions)\n",
    "        metric_2.update_state(labels, predictions)\n",
    "        metric_3.update_state(labels, predictions)\n",
    "        metric_4.update_state(labels, predictions)\n",
    "\n",
    "        return {\"loss\": loss, \n",
    "                \"accuracy\": metric_1.result(), \n",
    "                \"recall\": metric_2.result(), \n",
    "                \"precision\": metric_3.result(), \n",
    "                \"topK15\": metric_4.result()}\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [metric_1, metric_2, metric_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "Gm96WaUdCJlp"
   },
   "outputs": [],
   "source": [
    "def _parse_function(example_proto):\n",
    "\n",
    "    feature_description = {\n",
    "        'paper_title': tf.io.RaggedFeature(tf.int64),\n",
    "        'paper_mask': tf.io.RaggedFeature(tf.int64),\n",
    "        'journal': tf.io.FixedLenFeature((1,), tf.int64),\n",
    "        'doc_type': tf.io.FixedLenFeature((1,), tf.int64),\n",
    "        'targets': tf.io.FixedLenFeature((20,), tf.int64)\n",
    "    }\n",
    "\n",
    "    example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "    paper_title = example['paper_title']\n",
    "    paper_mask = example['paper_mask']\n",
    "    doc_type = example['doc_type']\n",
    "    journal = example['journal']\n",
    "    targets = example['targets']\n",
    "\n",
    "    return (paper_title, paper_mask, doc_type, journal), targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "KRzDLBiuAvWO"
   },
   "outputs": [],
   "source": [
    "def get_dataset(path, num_parts, batch_size, data_type='train'):\n",
    "    \n",
    "    tfrecords = [f\"{path}{data_type}/{x}\" for x in os.listdir(f\"{path}{data_type}/\") if x.endswith('tfrecord')]\n",
    "    \n",
    "    raw_dataset = tf.data.TFRecordDataset(tfrecords, num_parallel_reads=AUTO)\n",
    "\n",
    "    parsed_dataset = raw_dataset.map(_parse_function, num_parallel_calls=AUTO)\n",
    "\n",
    "    parsed_dataset = parsed_dataset \\\n",
    "    .apply(tf.data.experimental.dense_to_ragged_batch(256, drop_remainder=True)).shuffle(2048)\n",
    "    return parsed_dataset.prefetch(AUTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "ifzXv0UlOwRB"
   },
   "outputs": [],
   "source": [
    "file_path = f'./{model_iteration}/tfrecords/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "oyogs8vmzpOn"
   },
   "outputs": [],
   "source": [
    "# train_ds = get_dataset(file_path, 30, 256, 'train')\n",
    "val_ds = get_dataset(file_path, 1, 256, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
     ]
    }
   ],
   "source": [
    "mirrored_strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "o8CgRmlezpKC",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at albert-base-v2 were not used when initializing TFAlbertModel: ['predictions']\n",
      "- This IS expected if you are initializing TFAlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFAlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFAlbertModel were initialized from the model checkpoint at albert-base-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    }
   ],
   "source": [
    "with mirrored_strategy.scope():\n",
    "  \n",
    "    model = TFAlbertModel.from_pretrained('albert-base-v2')\n",
    "    model.layers[0].trainable = False\n",
    "\n",
    "    # Model Inputs\n",
    "    paper_title_input_ids = tf.keras.layers.Input((512,), dtype=tf.int64, name='paper_title_ids')\n",
    "    paper_title_att_mask = tf.keras.layers.Input((512,), dtype=tf.int64, name='paper_title_mask')\n",
    "    doc_type_id = tf.keras.layers.Input((1,), dtype=tf.int64, name='doc_type_id')\n",
    "    journal_id = tf.keras.layers.Input((1,), dtype=tf.int64, name='journal_id')\n",
    "\n",
    "    # Using HF Model for Title Representation\n",
    "    paper_title_embs = model(input_ids = paper_title_input_ids, \n",
    "                             attention_mask=paper_title_att_mask, \n",
    "                             output_hidden_states=True, \n",
    "                             training=False).last_hidden_state\n",
    "\n",
    "    # Embedding Layers\n",
    "    #     paper_title_embs = tf.keras.layers.Embedding(input_dim=50258, \n",
    "    #                                                  output_dim=512, \n",
    "    #                                                  mask_zero=False, \n",
    "    #                                                  trainable=True,\n",
    "    #                                                  name=\"title_embedding\")(paper_title_input_ids)\n",
    "\n",
    "    doc_embs = tf.keras.layers.Embedding(input_dim=len(doc_vocab)+1, \n",
    "                                          output_dim=32, \n",
    "                                          mask_zero=False, \n",
    "                                          name=\"doc_type_embedding\")(doc_type_id)\n",
    "\n",
    "    journal_embs = tf.keras.layers.Embedding(input_dim=len(journal_vocab)+1, \n",
    "                                              output_dim=128, \n",
    "                                              mask_zero=False, \n",
    "                                              name=\"journal_embedding\")(journal_id)\n",
    "\n",
    "    # First layer\n",
    "    dense_output = tf.keras.layers.Dense(1024, activation='relu', \n",
    "                                         kernel_regularizer='L2', name=\"dense_1\")(paper_title_embs)\n",
    "    dense_output = tf.keras.layers.Dropout(0.20, name=\"dropout_1\")(dense_output)\n",
    "    dense_output = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"layer_norm_1\")(dense_output)\n",
    "    dense_output_flat = tf.keras.layers.GlobalAveragePooling1D(name=\"title_pooling_layer\")(dense_output)\n",
    "    doc_flat = tf.keras.layers.GlobalAveragePooling1D(name=\"doc_pooling_layer\")(doc_embs)\n",
    "    journal_flat = tf.keras.layers.GlobalAveragePooling1D(name=\"journal_pooling_layer\")(journal_embs)\n",
    "    concat_output = tf.concat(values=[dense_output_flat, journal_flat, doc_flat], axis=1)\n",
    "\n",
    "    # Second layer\n",
    "    dense_output = tf.keras.layers.Dense(1024, activation='relu', \n",
    "                                         kernel_regularizer='L2', name=\"dense_2\")(concat_output)\n",
    "    dense_output = tf.keras.layers.Dropout(0.20, name=\"dropout_2\")(dense_output)\n",
    "    dense_output = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"layer_norm_2\")(dense_output)\n",
    "\n",
    "    # Third Layer\n",
    "    dense_output = tf.keras.layers.Dense(256, activation='relu', \n",
    "                                         kernel_regularizer='L2', name=\"dense_3\")(dense_output)\n",
    "    dense_output = tf.keras.layers.Dropout(0.20, name=\"dropout_3\")(dense_output)\n",
    "    dense_output = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"layer_norm_3\")(dense_output)\n",
    "    dense_output_flat = tf.keras.layers.GlobalAveragePooling1D(name=\"title_pooling_layer\")(dense_output)\n",
    "\n",
    "\n",
    "    # Output Layer\n",
    "    final_output = tf.keras.layers.Dense(len(target_vocab)+1, activation=\"sigmoid\", \n",
    "                                         name=\"cls\")(dense_output_flat)\n",
    "\n",
    "    test_model = CustomModel(inputs=[paper_title_input_ids, paper_title_att_mask, doc_type_id, journal_id], \n",
    "                             outputs=final_output, name='test_model')\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "RF9EaGRSFwNA"
   },
   "outputs": [],
   "source": [
    "test_model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"test_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "paper_title_ids (InputLayer)    [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "paper_title_mask (InputLayer)   [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_albert_model_7 (TFAlbertMode TFBaseModelOutputWit 11683584    paper_title_ids[0][0]            \n",
      "                                                                 paper_title_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512, 1024)    787456      tf_albert_model_7[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512, 1024)    0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_norm_1 (LayerNormalizatio (None, 512, 1024)    2048        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512, 1024)    1049600     layer_norm_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512, 1024)    0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_norm_2 (LayerNormalizatio (None, 512, 1024)    2048        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 512, 256)     262400      layer_norm_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 512, 256)     0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_norm_3 (LayerNormalizatio (None, 512, 256)     512         dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "title_pooling_layer (GlobalAver (None, 256)          0           layer_norm_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "doc_type_id (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "journal_id (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cls (Dense)                     (None, 388176)       99761232    title_pooling_layer[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 113,548,880\n",
      "Trainable params: 101,865,296\n",
      "Non-trainable params: 11,683,584\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "_sVDz17xl1sr"
   },
   "outputs": [],
   "source": [
    "callbacks = [tf.keras.callbacks.ModelCheckpoint(f'./models/{model_iteration}/{model_iteration}_first_try', \n",
    "                                                save_best_only=False, save_weights_only=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First try (with all variables and Albert model output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HCHnQhCYzpEe",
    "outputId": "fbb679d0-029c-4b28-9d8e-37e5a643df53",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "INFO:tensorflow:batch_all_reduce: 14 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 14 all-reduces with algorithm = nccl, num_packs = 1\n",
      "74215/74215 [==============================] - 9759s 131ms/step - loss: 6.2373 - accuracy: 0.3410 - recall: 0.0135 - precision: 0.0280 - val_loss: 4.1578 - val_accuracy: 0.4481 - val_recall: 0.0933 - val_precision: 0.5296\n",
      "INFO:tensorflow:Assets written to: ./models/baseline/first_try/assets\n",
      "Epoch 2/2\n",
      "74215/74215 [==============================] - 9745s 131ms/step - loss: 3.8262 - accuracy: 0.4863 - recall: 0.0540 - precision: 0.7250 - val_loss: 4.6855 - val_accuracy: 0.3344 - val_recall: 0.1575 - val_precision: 0.4799\n",
      "INFO:tensorflow:Assets written to: ./models/baseline/first_try/assets\n"
     ]
    }
   ],
   "source": [
    "history = test_model.fit(train_ds, epochs=5, validation_data=val_ds, verbose=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYmWR7c4zo95"
   },
   "source": [
    "## ARCHIVE: Baseline Second Try (trainable embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "kB2ZrZJYzozE",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "INFO:tensorflow:batch_all_reduce: 14 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:batch_all_reduce: 14 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "74215/74215 [==============================] - 12309s 165ms/step - loss: 6.1006 - accuracy: 0.3679 - recall: 0.0327 - precision: 0.0937 - val_loss: 4.5469 - val_accuracy: 0.4830 - val_recall: 0.0926 - val_precision: 0.5677\n",
      "INFO:tensorflow:Assets written to: ./models/baseline/second_try/assets\n",
      "Epoch 2/5\n",
      "74215/74215 [==============================] - 12302s 166ms/step - loss: 3.7602 - accuracy: 0.4981 - recall: 0.0616 - precision: 0.7264 - val_loss: 3.9754 - val_accuracy: 0.3812 - val_recall: 0.1413 - val_precision: 0.4945\n",
      "INFO:tensorflow:Assets written to: ./models/baseline/second_try/assets\n",
      "Epoch 3/5\n",
      "74215/74215 [==============================] - 12298s 166ms/step - loss: 3.6041 - accuracy: 0.4553 - recall: 0.0732 - precision: 0.7266 - val_loss: 3.7417 - val_accuracy: 0.3581 - val_recall: 0.1575 - val_precision: 0.5284\n",
      "INFO:tensorflow:Assets written to: ./models/baseline/second_try/assets\n",
      "Epoch 4/5\n",
      "74215/74215 [==============================] - 12300s 166ms/step - loss: 3.5382 - accuracy: 0.4361 - recall: 0.0789 - precision: 0.7297 - val_loss: 4.0600 - val_accuracy: 0.3269 - val_recall: 0.1709 - val_precision: 0.5320\n",
      "INFO:tensorflow:Assets written to: ./models/baseline/second_try/assets\n",
      "Epoch 5/5\n",
      "74215/74215 [==============================] - 12299s 166ms/step - loss: 3.5007 - accuracy: 0.4254 - recall: 0.0824 - precision: 0.7321 - val_loss: 3.9740 - val_accuracy: 0.3209 - val_recall: 0.1731 - val_precision: 0.5413\n",
      "INFO:tensorflow:Assets written to: ./models/baseline/second_try/assets\n"
     ]
    }
   ],
   "source": [
    "history = test_model.fit(train_ds, epochs=5, validation_data=val_ds, verbose=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "mag_model_baseline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
